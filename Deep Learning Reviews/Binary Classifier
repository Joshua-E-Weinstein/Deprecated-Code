print("Importing Libraries..........")

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
from json import loads

!pip install tensorflow_text
import tensorflow_text as text
from official.nlp import optimization

import re

import math
import random

# gpus = tf.config.list_physical_devices('GPU')
# if gpus:
#   try:
#     # Currently, memory growth needs to be the same across GPUs
#     for gpu in gpus:
#       tf.config.experimental.set_memory_growth(gpu, True)
#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')
#     print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
#   except RuntimeError as e:
#     # Memory growth must be set before GPUs have been initialized
#     print(e)

print("Preparing..........")

maxRating = 5
batchSize = 32
highestRating = 5.0

# Uses regex to clean the reviews.
def preprocess_text(sen):

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

# data = input("Name of the Json file in Data: ")
# output = input("Name of file to save model as in Models folder (No extension): ")

print("Building Model..........")

# appliancesData = open("content/{}".format(data), 'r')  # Opens data file.
appliancesData = open("drive/MyDrive/Train/Office_Products_5.json", 'r')

dataTable = [loads(line) for line in appliancesData]  # Converts each line of data into a dictionary.

# Makes list of tuples with the reviews tied to their ratings.
reviews = list()
ratings = list()

for review in dataTable[0:500]:
    if "summary" in review:
        reviews.append(review["summary"] + " " + review.get("reviewText", ""))
        rating = review["overall"]

        if rating >= 2.5:
          ratings.append(1)
        else:
          ratings.append(0)
        

cleanReviews = list()

cleanReviews = []
sentences = reviews
for sen in sentences:
    cleanReviews.append(preprocess_text(sen))

print(cleanReviews)
print(ratings)

length = len(ratings)
trainReviews, validationReviews, testReviews = np.split(tf.convert_to_tensor(tf.constant(cleanReviews)), [round(length*0.3), round(length*0.5)])
trainRatings, validationRatings, testRatings = np.split(tf.convert_to_tensor(tf.constant(ratings)), [round(length*0.3), round(length*0.5)])


trainDataset, validationDataset, testDataset = tf.data.Dataset.from_tensor_slices((trainReviews, trainRatings)), tf.data.Dataset.from_tensor_slices((validationReviews, validationRatings)), tf.data.Dataset.from_tensor_slices((testReviews, testRatings))

print(trainDataset)


# Makes first layer. Used to convert text into numbers.
preProcess = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3", 
                            name='preprocessing')
encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1", 
                            trainable=True, 
                            name='BERT_encoder')

# encoderInput = preProcess(cleanReviews)
# encoderOutput = (encoder(encoderInput)['pooled_output'])
# print(encoderOutput)

def buildModel():
  textInput = tf.keras.layers.Input(shape=(), dtype=tf.string, name='inputs')
  preProcessingLayer = preProcess
  encoderInputs = preProcessingLayer(textInput)
  encoderLayer = encoder
  encoderOutputs = encoder(encoderInputs)

  net = encoderOutputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(textInput, net)

model = buildModel()

rawResults = model(tf.constant([cleanReviews[0]]))
print(tf.sigmoid(rawResults))

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()

epochs = 10
epochSteps = tf.data.experimental.cardinality(trainDataset).numpy()
trainSteps = epochSteps * epochs
warmupSteps = int(0.1*trainSteps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr, 
                                          num_train_steps=trainSteps, 
                                          num_warmup_steps=warmupSteps, 
                                          optimizer_type='adamw')

model.compile(optimizer=optimizer, 
              loss=loss, 
              metrics=metrics)

print("Training..........")
history = model.fit(x=trainDataset.batch(batchSize), 
                    validation_data=validationDataset.batch(batchSize), 
                    epochs = epochs)
