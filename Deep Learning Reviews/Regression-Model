print("Importing Libraries..........")

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
from json import loads

!pip install tensorflow_text
import tensorflow_text as text
# from official.nlp import optimization

import re

import math
import random
from time import time

# gpus = tf.config.list_physical_devices('GPU')
# if gpus:
#   try:
#     # Currently, memory growth needs to be the same across GPUs
#     for gpu in gpus:
#       tf.config.experimental.set_memory_growth(gpu, True)
#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')
#     print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
#   except RuntimeError as e:
#     # Memory growth must be set before GPUs have been initialized
#     print(e)

print("Preparing..........")

maxRating = 5
batchSize = 64
highestRating = 5.0

# Uses regex to clean the reviews.
def preprocess_text(sen):

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sen)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

# data = input("Name of the Json file in Data: ")
# output = input("Name of file to save model as in Models folder (No extension): ")

print("Building Model..........")

# appliancesData = open("content/{}".format(data), 'r')  # Opens data file.
appliancesData = open("drive/MyDrive/Train/Office_Products_5.json", 'r')

dataTable = [loads(line) for line in appliancesData]  # Converts each line of data into a dictionary.

# Makes list of tuples with the reviews tied to their ratings.
reviews = list()
ratings = list()

balanceNum = 200
balanceCount = [0] * maxRating
fullList = [balanceNum] * maxRating
for review in dataTable:
    if balanceCount == fullList:
        break
    elif "overall" in review and "summary" in review:
        if balanceCount[int(review["overall"])-1] == balanceNum:
            continue
        else:
            balanceCount[int(review["overall"]) - 1] += 1

            reviews.append(review["summary"] + " " + review.get("reviewText", ""))
            rating = review["overall"]/maxRating

            # ratings.append(rating)
            ratings.append(rating)

constSeed = time()
random.seed(constSeed)
random.shuffle(reviews)
random.seed(constSeed)
random.shuffle(ratings)

cleanReviews = list()

cleanReviews = []
sentences = reviews
for sen in sentences:
    cleanReviews.append(preprocess_text(sen))

print(cleanReviews)
print(ratings)

length = len(ratings)
trainReviews, validationReviews, testReviews = np.split(tf.convert_to_tensor(tf.constant(cleanReviews)), [round(length*0.3), round(length*0.5)])
trainRatings, validationRatings, testRatings = np.split(tf.convert_to_tensor(tf.constant(ratings)), [round(length*0.3), round(length*0.5)])


trainDataset, validationDataset, testDataset = tf.data.Dataset.from_tensor_slices((trainReviews, trainRatings)), tf.data.Dataset.from_tensor_slices((validationReviews, validationRatings)), tf.data.Dataset.from_tensor_slices((testReviews, testRatings))

print(trainDataset)


# Makes first layer. Used to convert text into numbers.
preProcess = hub.KerasLayer("gs://deeplearner55/bert_en_uncased_preprocess_3", 'r', 
                            name='preprocessing')
encoder = hub.KerasLayer("gs://deeplearner55/small_bert_bert_en_uncased_L-4_H-512_A-8_2", 'r',  
                            name='BERT_encoder')

# encoderInput = preProcess(cleanReviews)
# encoderOutput = (encoder(encoderInput)['pooled_output'])
# print(encoderOutput)

def buildModel():
  textInput = tf.keras.layers.Input(shape=(), dtype=tf.string, name='inputs')
  preProcessingLayer = preProcess
  encoderInputs = preProcessingLayer(textInput)
  encoderLayer = encoder
  encoderOutputs = encoder(encoderInputs)

  net = encoderOutputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(64, activation='relu', name='dense')(net)
  net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)
  return tf.keras.Model(textInput, net)

model = buildModel()

rawResults = model(tf.constant([cleanReviews[0]]))
print(tf.sigmoid(rawResults))

loss = tf.keras.losses.MeanSquaredError()
# metrics = tf.metrics.Accuracy()

epochs = 30
# epochSteps = tf.data.experimental.cardinality(trainDataset).numpy()
# trainSteps = epochSteps * epochs
# warmupSteps = int(0.1*trainSteps)

# init_lr = 3e-5
# optimizer = optimization.create_optimizer(init_lr=init_lr, 
#                                           num_train_steps=trainSteps, 
#                                           num_warmup_steps=warmupSteps, 
#                                           optimizer_type='adamw')

model.compile(optimizer='adam', 
              loss=loss, 
              metrics=['accuracy'])

print("Training..........")
history = model.fit(x=trainDataset.batch(batchSize), 
                    validation_data=validationDataset.batch(batchSize), 
                    epochs = epochs)

# Saves the model.
# model.save('Models/{}'.format(output))
model.save("gs://deeplearner55/Models/testRegression1")

# Evaluates the model's accuracy and loss.
results = model.evaluate(testDataset.batch(batchSize), verbose=2)

# Shows results of evaluation.
print("Results:")
for name, value in zip(model.metrics_names, results):
    print("%s: %.3f" % (name, value))
